\section*{Problema}

Run-Length encoding es una forma de compresión de datos sin pérdida, 
donde la secuencia de símbolos de la entrada se almacenan 
como una simple ocurrencia con el número simbolos consecutivos.

Definimos, para una cadena $w \in \Sigma^{*}$, una serie de 
pares $(r,l,s)$ de datos consecutivos dadas por:
\begin{itemize}
    \item El carácter de control $r \notin  \Sigma$.
    \item La longitud de la secuencia $l \in \mathbb{N} $
    \item El símbolo del alfabeto $s \in \Sigma$
\end{itemize}

Por ejemplo, tenemos una cadena perteneciente al alfabeto en inglés, 
\texttt{ABABBBC} y la lista de tuplas con el caracter de 
control $\alpha$, se tiene:
\begin{align*}
    (\alpha,1,\texttt{A}), (\alpha,1,\texttt{B}), (\alpha,1,\texttt{A}) \\
    (\alpha,3,\texttt{B}), (\alpha,1,\texttt{C})
\end{align*} 

Para poder representarlo en un archivo, los símbolos con un solo elemento
consecutivo, es decir que no se repiten, se representarán como \texttt{s}. Con el
ejemplo anterior, la cadena comprimida sería \texttt{ABA($\alpha,3$,B)C}.

Por lo tanto, el algoritmo es eficiente para cadenas o contenido donde
hay muchas secuencias consecutivas, es decir, archivos de texto, binarios
representación de una imagen por medio de pixeles o componentes de bloques
largos de archivos de sonido. (\cite{pu2005fundamental}).

Para este trabajo, se consideran únicamente archivos de texto para comprimir
que estén en \texttt{utf-8}, debido a que, se usará una combinación para
representar las tuplas. Para los símbolos sin repetición o secuencia muy cortas,
, como se menciona en (\cite{pu2005fundamental}), se implementará 
un modo literal donde los bytes se escriben directamente sin tupla. 
Esto evita la expansión del archivo, permitiendo que la compresión se reserve
únicamente para las secuencias que si generen un ahorro significativo.

\subsection*{Algoritmo secuencial}

Como se menciona en (\cite{pu2005fundamental}), se define caracteres de
control de repetición $r_3,r_4, \ldots$, los cuales para $r_i$
representa la cantidad $i$ de símbolos consecutivos. De esta manera,
se usará el caracter de control únicamente cuando la longitud sea 
mayor a $3$.

Asi, definimos el algoritmo de encriptación como:
\begin{itemize}
    \item Repetir hasta llegar al final del archivo:
    \item[]{
        \begin{itemize}
            \item Leer el símbolo $S$ de la secuencia:
            \item[]{
                \begin{itemize}
                    \item{
                        Contar el número de símbolos consecutivos
                        iguales $i$ hasta encontrar uno diferente.
                    }
                    \item{
                        Guardar en el archivo de salida el par
                        $r_iS$ solo si $i > 2$, en oto caso guardar
                        el símbolo $i$
                    }
                \end{itemize}
            }
        \end{itemize}
    }
\end{itemize}

Para la implementación, se debe de considerar tres apuntadores $i,j$
donde $j = i + 1$  donde los usaremos para recorrer el arreglo de símbolos
$S$, para lo cual, mientras que $S[i] == S[j]$ entonces podemos incrementar
el contador, de esta manera verificamos si es mayor al umbral, de esta
manera podemos representarlo en forma de tupla y en otro caso, se escribe
el caracter $S[q]$ y $S[p]$ en la cadena comprimida. (El pseudocódigo
se encuentra en el anexo \ref{anexo:codigo.1})

De esta manera, definimos el algoritmo de desencriptación como:
\begin{itemize}
    \item Repetir hasta llegar al final del archivo:
    \item[]{
        \begin{itemize}
            \item Leer el símbolo $S$ de la secuencia:
            \item[]{
                \begin{itemize}
                    \item{
                        Si el símbolo $S = r_i$, se debe de leer el
                        símbolo consecutivo y escribir $i$ veces en
                        la salida.
                    }
                    \item{
                        En otro caso, escribir el símbolo actual.
                    }
                \end{itemize}
            }
        \end{itemize}
    }
\end{itemize}

De la misma manera, es necesario un apuntador $p$ para recorrer el archivo
comprimido, donde si $S[p]$ es el caracter de control definido $r$, se toma
el valor de $S[p+1] = n$ como un número y $S[p+2]$ como el símbolo a repetir
$n$ veces, en otro caso se guarda $S[p]$ en la salida. (El pseudocódigo
se encuentra en el anexo \ref{anexo:codigo.2})

A su vez, se considera el uso de \texttt{FLAG\_RLE} para iniciar la tupla
definida en la sección anterior y en dado caso que el símbolo $s$ sea igual
a \texttt{FLAG\_RLE}, se considera a \texttt{FLAG\_LITERAL} para escapar
y escribir el valor de \texttt{FLAG\_RLE}.

\subsection*{Algoritmo en paralelo}

Para esto, se toma en inspiración del artículo (\cite{manchev2006parallel}),
donde se propone el dividir la secuencia de entrada en bloques procesados
en paralelo donde cada proceso identifica las corridas locales dentro 
de su bloque, para posteriormente realizar una fase de combinación donde se
revisa si los límites entre bloques contiguos se cruzan con corridas
de símbolos, asegurando que las secuencias iguales se representen correctamente.

De esta manera, se debe de utilizar el uso de memoria compartidad y distribuida
para poder ejecutar el algoritmo y por lo tanto, nos esta danto la pista de usar
\texttt{MPI}.

Antes de pasar a la implementación, debemos mencionar que esta estrategía de
paralelizar el algoritmo secuencial \texttt{RLE} sigue el principio de 
\texttt{Fork Join} donde se divide el trabajo entre los distintos procesadores
y al final, juntar los resultados de cada procesador en uno global.

Se puede observar la técnica de \texttt{Fork Join} de manera explicita 
al dividir la secuencia de entrada en bloques para que cada procesador 
realice \texttt{RLE} de manera local y al final se junta cada bloque 
revisando si no hay cruzes entre cada uno.

De esta manera, proponemos el pseudocódigo (\ref{anexo:codigo.3}) donde se puede
observar la compresión en \texttt{RLE} en paralelo y en  (\ref{anexo:codigo.4})
la manera de descomprimir en paralelo.

Uno de los detalles al momento de realizar la implementación, es al momento
de recopilar el resultado de cada proceso, debido a la siguientes consideraciones
para sea un proceso $P_i, P_{i-1}$:

\begin{itemize}
    \item{
        Al momento de comprimir, si dos chunks de memoria contenian
        la misma secuencia de símbolos, el proceso $P_i$ debe sumar
        los caracteres de la frontera de $P_{i-1}$ y $P_{i-1}$ 
        debe de borrar del vector de resultado la información de la 
        representación compactada de la secuencia de símbolos.
    } 
    \item{
        Al momento de descomprimir, si la representación compactada
        de una secuencia de símbolos es dividida entre la frontera de
        dos chunks, el proceso $P_i$ debe de tomar la información
        faltante de $P_{i-1}$ para que $P_i$ pueda descomprimir c
        orrectamentela información. Para esto, se toma a lo más 
        2 bytes de del chunk anterior continuo y se elimina la 
        información tomada del chunk anterior.
    }
\end{itemize}

\section*{Experimentación}

Para esto, trabajaremos con 3 archivos de 100 MB con una alta, media y baja 
repetitividad los cuales se calculara el tiempo de comprimir de manera secuencial
y paralela con ${2,4,6,20,50}$ procesadores.

De este modo, se realizará la prueba en el siguiente equipo:
\begin{lstlisting}[style=BashStyle]
---------------------
Sistema operativo: Arch Linux
Kernel: 6.17.9-arch1-1 
Arquitectura: x86_64
CPU: Intel(R) Core(TM) i5-8250U CPU @ 1.60GHz
Nucleos fisicos: 4
Nucleos logicos: 8
Memoria RAM total: 16,00 GB 
-------------------------------------------
\end{lstlisting}

Para esto, se creo un script \verb|generate_test_data.sh|, el 
cual produce los siguientes archivos de \texttt{100 MB}.

\begin{enumerate}
    \item Un archivo de alta repetitividad (solo contiene un único tipo de byte, \texttt{A = 0x41}).
    \item Un archivo aleatorio que representa el peor caso para este algoritmo (tomando elementos aleatorios de \verb|/dev/urandom|).
    \item Un archivo de repetitividad media tipo malla, donde se repite el patrón \texttt{0123456789}.
\end{enumerate}

Por último, se creo un script para comprimir archivos por medio de 
$N$ procesos y de manera secuencial para medir la eficiencia y el
\texttt{Speedup}.

\section*{Resultados}

Se realizó la experimentación conforme a lo descrito en la sección anterior, teniendo
los siguientes tiempos para cada uno de los archivos y con los
siguientes detalles por cada caso (usando las gráficas 
\ref{fig:eficiencia_vs_procesos}, \ref{fig:speedup_vs_procesos} 
y \ref{fig:tiempo_vs_procesos}):

\begin{itemize}
    \item{
        Para \verb|data_plana.bin|, por la tabla \ref{tab:plana} se puede
        observar que el mejor tiempo promedio fue cuando se utilizo $N=6$
        procesos con $0.0701$ y con un Speedup de $S = 1.9$ lo cual
        está lejos de lo ideal ya que tuve que mejorar 6 veces. Por último,
        su eficiencia, disminuyo cerca del $30\%$ pero en el caso de
        $N=4$ su eficiencia es de $46\%$ lo cual es optimo.
    }
    \item{
        Para \verb|data_malla.bin|,por la tabla \ref{tab:malla} llega a su menor tiempo promedio con
        $N=4$ con $0.2123$ y un Speedup alrededor de $S=2.5$, aunque
        está un poco más de lo idea ya que tiene que mejorar 4 veces pero
        es suficiente. De esta manera, con respecto a su eficiencia se tiene
        que es de $62\%$, lo cual no dismninuye mucho. 
    }
    \item{
        Para \verb|data_aleatoria.bin|, por la tabla \ref{tab:aleatoria}
        llega con su mejor tiempo promedio con $N=4$ con $0.2324$ y un
        Speedup alrededor de $2.4200$, lo cual como en los dos casos anteriores,
        es más de la mitad al esperarse que mejore 4 veces. De esta manera,
        con respecto a su eficiencia es del $60\%$, lo cual no disminuye mucho.
     }
\end{itemize}

Con lo anterior dicho y revisando las gráficas, podemos observar que el número
de proceso optimos para el equipo en que se realizaron las pruebas es de
$N=4$ al tener en su mayría de los casos el mejor tiempo proemdio, Speedup
y eficiencia. 

De similar manera, podemos ver que al incrementar el número de procesos
mayor a $4$, empieza a incrementar el tiempo promedio entonces no se
beneficia del incremento de los procesos, ya que al realizarse cambio de
contexto se ralentiza el proyecto.

Otra observación, es al momento de comunicar los resultados y verificando
las fronteras entre procesos continuos para evitar que la compresión no 
sea correcta al no considerar secuencias continuas entre dos chunks
o se pierda información en la descompresión al dividir el archivo 
en chunks y que se pierda información como tuplas de información.

De esta manera, el uso de computo distribuido para trabajar el algoritmo
paralelo por medio de la técnica de \texttt{Fork Join} aunque mejora los
tiempos, a la larga no se beneficia por la cantidad de procesos. Por último, 
la parte más complicada de la implementación de esta versión de
\texttt{RLE} fue la unión de los datos y la comunicación entre procesos. 

\section*{Conclusión}

Como podemos observar durante el reporte, la realización de un algoritmo 
secuencial a paralelo aunque en ciertas ocasiones es fácil como 
en \texttt{RLE}, su implementación es de las más criticas porque 
se debe de considerar condiciones para combinar el resultado 
de cada proceso y obtener la salida correctamente como el algoritmo secuencial.

De esta manera, en lo personal se considero un reto bastante técnico
y de implementación al considerar el uso de computo distribuido 
como uno de los ejes principales del proyecto junto con la técnica de
\texttt{Fork Join} para paralelizar \texttt{RLE}.

A su vez, al trabajar con poco con \texttt{C++}, el aprender la sintaxis 
y la manera de usar \texttt{MPI} fue de lo más complicado y era de lo mejor
para evitar el manejo de memoria que se debe d e considerar en una 
implementación en \texttt{C} y el uso de programación orientada a objetos
ya incluida en \texttt{C++} facilitando la implementación.

Por último, con la visualización de los datos, podemos entender que
la implementación mejoro significativamente el tiempo del algoritmo
alcanzando su menor tiempo promedio con $N=4$ procesos lo cual para
datos enormes de memoria, se puede considerar mejor alternativa al 
la implementación secuencial.



